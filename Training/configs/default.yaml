default_device: "cuda:1" # This is the default device for the training. If you want to use a different device, you can change it here

train:
  use_low_level: true         # <--- true = use manual loop
  batch_size: 3
  eval_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
  num_epochs: 1
  phase: "mitigations" # This will be programatically changed to "playbook" and "full" during the curriculum training
  mixed_precision: "bf16" # Put "no" if you want to use full precision
  fp16: false
  save_total_limit: 2
  seed: 42
  save_steps: 50
  eval_steps: 50
  logging_steps: 20
  max_steps: 100000
  gradient_checkpointing: false # DISABLE THIS WHEN ENOUGH MEMORY
  warmup_steps: 10 # Number of warmup steps for learning rate scheduler
  weight_decay: 0.01
  save_best_only: false # If true, the training will only save the best checkpoint
  do_not_save_interrupted: true # If true, the training will not save the last checkpoint if it was interrupted
  no_save_during_testing: true # If true, there will be no saving while running with dataset.subset_mode true
  max_grad_norm: 1.0 # Gradient clipping, 0.5 is a good value for the dataset size

dataset:
  max_token_length: 2048
  train_path: "Dataset/Main/dataset_train.json"
  val_path: "Dataset/Main/dataset_val.json"
  subset_mode: false # PUT THIS FOR DEBUGGING . IT WILL RUN ON A VERY SMALL SUBSET
  debug_keep_text: false
  debug_single_processor_mode: true

model:
  name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
  apply_lora: true
  lora_r: 8
  lora_alpha: 32
  use_only_cpu: false

logging:
  use_wandb: false
  wandb_run_id : null
  slurm_id: null
  save_dir: "outputs"
  wandb_run_file_path: "Training/configs/wandb_run_id.txt"

