train:
  use_low_level: true         # <--- true = use manual loop
  batch_size: 1
  eval_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  num_epochs: 3
  phase: "mitigations"
  mixed_precision: "bf16" # Put "no" if you want to use full precision
  fp16: false
  save_total_limit: 2
  seed: 42
  save_steps: 500
  eval_steps: 500
  logging_steps: 500
  gradient_checkpointing: false # DISABLE THIS WHEN ENOUGH MEMORY
  warmup_steps: 100 # Number of warmup steps for learning rate scheduler
  weight_decay: 0.01


dataset:
  max_token_length: 2048
  train_path: "Dataset/Main/dataset_train.json"
  val_path: "Dataset/Main/dataset_val.json"
  subset_mode: true
  debug_keep_text: false
  debug_single_processor_mode: true

model:
  name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
  apply_lora: true
  lora_r: 8
  lora_alpha: 32
  use_only_cpu: false

logging:
  use_wandb: false
  wandb_run_id : null
  slurm_id: null
  save_dir: "outputs"
  wandb_run_file_path: "Training/configs/wandb_run_id.txt"