{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb30abaca8b91def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:08.555191Z",
     "start_time": "2025-04-27T11:22:04.325953Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import TypedDict\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import json\n",
    "import json\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import json\n",
    "import argparse\n",
    "from Utils import DynamicAttributes\n",
    "\n",
    "# Some global variables\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MAX_TOKENS_LENGTH = 8192\n",
    "\n",
    "# Model and tokenizer variables that will be set later and used in the script\n",
    "tokenizer = None\n",
    "model = None\n",
    "train_data = None\n",
    "val_data = None\n",
    "\n",
    "gParams = DynamicAttributes()\n",
    "\n",
    "# =====================\n",
    "# Load and format dataset\n",
    "# =====================\n",
    "def load_flat_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    return [list(e.values())[0] for e in raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f868092698511b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:08.561359Z",
     "start_time": "2025-04-27T11:22:08.558445Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Tokenizer and label masking ===\n",
    "def tokenize_with_labels(example, mode=\"full\"):\n",
    "    text = example[\"text\"]\n",
    "    tokens = tokenizer(text, truncation=True, padding=\"max_length\", max_length=2048)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "    # Find where the target starts and ends. We basically mask out the prefix and suffix\n",
    "    if mode == \"mitigations\":\n",
    "        start = text.find(\"### Predicted Mitigations:\")\n",
    "        end = text.find(\"### Generated CACAO Playbook:\")\n",
    "    elif mode == \"playbook\":\n",
    "        start = text.find(\"### Generated CACAO Playbook:\")\n",
    "        end = len(text)\n",
    "    else:\n",
    "        start = 0\n",
    "        end = len(text)\n",
    "\n",
    "    # Tokenize the prefix and suffix\n",
    "    tokenized_prefix = tokenizer(text[:start], truncation=True, max_length=MAX_TOKENS_LENGTH)[\"input_ids\"]\n",
    "    tokenized_suffix = tokenizer(text[:end], truncation=True, max_length=MAX_TOKENS_LENGTH)[\"input_ids\"]\n",
    "\n",
    "    # Create labels and mask the prefix and suffix\n",
    "    labels = input_ids.copy()\n",
    "    labels[:len(tokenized_prefix)] = [-100] * len(tokenized_prefix)\n",
    "    labels[len(tokenized_suffix):] = [-100] * (len(labels) - len(tokenized_suffix))\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d413ee25a741b74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:08.625186Z",
     "start_time": "2025-04-27T11:22:08.622245Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# =====================\n",
    "# Load model and tokenizer\n",
    "# =====================\n",
    "def load_model_and_tokenizer():\n",
    "    global tokenizer\n",
    "    global model\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Apply LoRA - maybe in params in the future ?\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Force model to bf16\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "def parse_args():\n",
    "    global gParams\n",
    "    # =====================\n",
    "    # Configurable CLI\n",
    "    # =====================\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--phase\", type=str, default=\"full\", choices=[\"mitigations\", \"playbook\", \"full\"],\n",
    "                        help=\"Which task phase to train on\")\n",
    "    args = parser.parse_args()\n",
    "    gParams.finetune_mode = args.phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d00f6159ebfaae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:08.672187Z",
     "start_time": "2025-04-27T11:22:08.669273Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_datasets():\n",
    "    global train_data\n",
    "    global val_data\n",
    "    global train_data_subset\n",
    "\n",
    "    # =====================\n",
    "    # Load data and split by technique\n",
    "    # =====================\n",
    "    train_raw = load_flat_json(\"Dataset/Main/dataset_train.json\")\n",
    "    val_raw = load_flat_json(\"Dataset/Main/dataset_val.json\")\n",
    "\n",
    "    if gParams.subset_mode:\n",
    "        print(\"âš¡ Subsetting data for quick testing...\")\n",
    "        train_raw = train_raw[:100]  # Keep only 100 examples for fast training\n",
    "        val_raw = val_raw[:20]  # Keep 20 examples for fast validation\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = Dataset.from_list([format_for_completion(e) for e in train_raw])\n",
    "    val_dataset = Dataset.from_list([format_for_completion(e) for e in val_raw])\n",
    "\n",
    "    print(f\"Train size: {len(train_dataset)}\")\n",
    "    print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "    train_data = train_dataset.map(lambda ex: tokenize_with_labels(ex, mode=gParams.finetune_mode))\n",
    "    val_data = val_dataset.map(lambda ex: tokenize_with_labels(ex, mode=gParams.finetune_mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:08.721804Z",
     "start_time": "2025-04-27T11:22:08.718659Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class SaveModelAtEpochEndCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving model at end of epoch {state.epoch}\")\n",
    "        control.should_save = True\n",
    "        return control\n",
    "\n",
    "def train_model(subset=False):\n",
    "    # =====================\n",
    "    # Training setup\n",
    "    # =====================\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./llama3-cacao-checkpoints-{gParams.finetune_mode}\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        load_best_model_at_end=True,          # Reload best model based on metric\n",
    "        metric_for_best_model=\"eval_loss\",    # Use eval loss to pick best model\n",
    "        greater_is_better=False,              # Lower eval loss = better\n",
    "        fp16=True,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[SaveModelAtEpochEndCallback()],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(f\"./llama3-cacao-final-{gParams.finetune_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9306d68bf3baf0d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:10.098190Z",
     "start_time": "2025-04-27T11:22:08.769653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b133cd8c1d7d40f1b0587228e2b7ba0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200e5f15f5df464b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:10.123287Z",
     "start_time": "2025-04-27T11:22:10.119262Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Format dataset into prompt + completion style ===\n",
    "def format_for_completion(example):\n",
    "    if not isinstance(example, dict):\n",
    "        raise TypeError(\"Example must be a dictionary\")\n",
    "\n",
    "    if len(example) == 0: # In this case the format is { incident_id : example}\n",
    "        #print (f\"Formatting example: {example}\")\n",
    "        incident_id = list(example.keys())[0]\n",
    "        example = example[incident_id]\n",
    "\n",
    "    incident = example.get(\"incident_description\", \"\")\n",
    "    logs = example.get(\"attack_logs\", [])\n",
    "    mitigations = example.get(\"ground_truth_mitigations\", [])\n",
    "    playbook = example.get(\"playbook\", \"\")\n",
    "\n",
    "    logs_text = \"\\n\".join([\n",
    "        f\"- [{log['timestamp']}] {log['host']}: {log['action']} â€” {log['details']}\"\n",
    "        for log in logs if all(k in log for k in [\"timestamp\", \"host\", \"action\", \"details\"])\n",
    "    ])\n",
    "\n",
    "\n",
    "    def dict_to_str(d: dict, level=0) -> str:\n",
    "        part_text = \"\"\n",
    "        sep_char = \"; \" if level == 0 else \", \"\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, str):\n",
    "                part_text += f\"{key}: {value}\" + \"; \"\n",
    "            elif isinstance(value, list):\n",
    "                part_text += f\"{key}: \"\n",
    "                for item in value:\n",
    "                    if isinstance(item, str):\n",
    "                        part_text += f\"{item}\" + \", \"\n",
    "                    if isinstance(item, dict):\n",
    "                        res = dict_to_str(item, level+1)\n",
    "                        part_text += f\"{res}\" + \", \"\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                part_text += f\"{key}: \"\n",
    "                part_text += \"{\"\n",
    "                part_text += dict_to_str(value, level+1)\n",
    "                part_text += \"}\"\n",
    "\n",
    "        return part_text\n",
    "\n",
    "\n",
    "    def format_mitigation(mitigations_list: list[str | dict]) -> str:\n",
    "        assert isinstance(mitigations_list, list), \"Mitigations should be a list of strings or dicts\"\n",
    "\n",
    "        mitigations_text_out = \"\"\n",
    "        for mitigation in mitigations_list:\n",
    "            if isinstance(mitigations_list, str):\n",
    "                mitigations_text_out += f\"- {mitigation}\\n\"\n",
    "            elif isinstance(mitigation, dict):\n",
    "                mitigations_text_partial = \"- \"\n",
    "\n",
    "                res = dict_to_str(mitigation, 0)\n",
    "                mitigations_text_out += f\"{res}\\n\"\n",
    "\n",
    "        return mitigations_text_out\n",
    "\n",
    "    # Format mitigations\n",
    "    mitig_text = format_mitigation(mitigations)\n",
    "    playbook_text = json.dumps(playbook, indent=2)\n",
    "\n",
    "    full_text = f\"\"\"### Incident:\n",
    "{incident}\n",
    "\n",
    "### Logs:\n",
    "{logs_text}\n",
    "\n",
    "### Predicted Mitigations:\n",
    "{mitig_text}\n",
    "\n",
    "### Generated CACAO Playbook:\n",
    "{playbook_text}\"\"\"\n",
    "\n",
    "    return {\"text\": full_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198b178aa8f0af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:10.750124Z",
     "start_time": "2025-04-27T11:22:10.187395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Subsetting data for quick testing...\n",
      "Train size: 100\n",
      "Validation size: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaf880fcfea4812a18025d2a77edca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce7546fc4e4ff08355a53390b85309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "gParams.finetune_mode = \"mitigations\" # Set the default mode to \"mitigations\"\n",
    "gParams.subset_mode = True\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dfd29243cc2d1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:22:40.684076Z",
     "start_time": "2025-04-27T11:22:10.756057Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 191\u001B[39m\n\u001B[32m    188\u001B[39m     plt.grid(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    189\u001B[39m     plt.show()\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m \u001B[43mtrain_model_lowlevel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m#, gpus_list=gpus_list)\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 80\u001B[39m, in \u001B[36mtrain_model_lowlevel\u001B[39m\u001B[34m(subset, learning_rate, gradient_accumulation_steps, num_epochs, batch_size_train, batch_size_eval)\u001B[39m\n\u001B[32m     75\u001B[39m     accelerator = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     77\u001B[39m \u001B[38;5;66;03m# =====================\u001B[39;00m\n\u001B[32m     78\u001B[39m \u001B[38;5;66;03m# Dataloaders\u001B[39;00m\n\u001B[32m     79\u001B[39m \u001B[38;5;66;03m# =====================\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m train_loader = DataLoader(\u001B[43mtrain_data\u001B[49m, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m, batch_size=batch_size_train, collate_fn=custom_collate_fn)\n\u001B[32m     81\u001B[39m val_loader = DataLoader(val_data, shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size=batch_size_eval, collate_fn=custom_collate_fn)\n\u001B[32m     83\u001B[39m \u001B[38;5;66;03m# =====================\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[38;5;66;03m# Prepare model, optimizer and scheduler depending on accelerator usage\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# =====================\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "\n",
    "def is_running_in_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "USE_ACCELERATE = not is_running_in_notebook() # Turn this on to use accelerate\n",
    "\n",
    "\n",
    "def save_checkpoint(model, tokenizer, save_dir, accelerator=None):\n",
    "    if accelerator:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(save_dir, save_function=accelerator.save)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        accelerator.print(f\"Model checkpoint saved at {save_dir}\")\n",
    "    else:\n",
    "        # Standard pytorch save\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Model checkpoint saved at {save_dir}\")\n",
    "\n",
    "def evaluate_lowlevel(model, val_loader, accelerator):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.detach().cpu())\n",
    "    losses = torch.cat(losses)\n",
    "    model.train()  # Important: switch back to train mode after evaluation\n",
    "    return losses.mean().item()\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Batch is a list of dictionaries\n",
    "    #print(f\"Type of a batch: \": type(batch))\n",
    "    \n",
    "    ipdb.set_trace()\n",
    "    new_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        new_batch[key] = torch.stack([torch.tensor(item[key]) for item in batch])\n",
    "    return new_batch\n",
    "\n",
    "def train_model_lowlevel(subset=False, learning_rate=2e-5, gradient_accumulation_steps=4, num_epochs=3,\n",
    "                         batch_size_train=1, batch_size_eval=1):#, gpus_list=\"0,1,2,3,4,5,6,7\"):\n",
    "    global model\n",
    "\n",
    "    # =====================\n",
    "    # Accelerator setup\n",
    "    # =====================\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus_list\n",
    "    if USE_ACCELERATE:\n",
    "        from accelerate import Accelerator\n",
    "        from accelerate.utils import set_seed\n",
    "        from torch.distributed.optim import ZeroRedundancyOptimizer\n",
    "\n",
    "        accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            mixed_precision=\"bf16\",  # or \"fp16\"\n",
    "        )\n",
    "    else:\n",
    "        accelerator = None\n",
    "\n",
    "    # =====================\n",
    "    # Dataloaders\n",
    "    # =====================\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size_train, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size_eval, collate_fn=custom_collate_fn)\n",
    "\n",
    "    # =====================\n",
    "    # Prepare model, optimizer and scheduler depending on accelerator usage\n",
    "    # =====================\n",
    "\n",
    "    if USE_ACCELERATE:\n",
    "        # 1. Accelerate model and dataloaders (model gets cast to bf16)\n",
    "        model, train_loader, val_loader = accelerator.prepare(model, train_loader, val_loader)\n",
    "\n",
    "        # 2. Create optimizer and scheduler\n",
    "        optimizer = ZeroRedundancyOptimizer(model.parameters(), optimizer_class=torch.optim.AdamW, lr=learning_rate)\n",
    "\n",
    "        total_steps = (len(train_loader) // gradient_accumulation_steps) * num_epochs\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "        # 3. Accelerate optimizer and scheduler\n",
    "        optimizer, scheduler = accelerator.prepare(optimizer, scheduler)\n",
    "    else:\n",
    "        assert torch.cuda.is_available(), \"CUDA is not available. Please use with GPU support.\"\n",
    "        model = model.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Define optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        total_steps = (len(train_loader) // gradient_accumulation_steps) * num_epochs\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "        torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n",
    "\n",
    "\n",
    "    # =====================\n",
    "    # Training loop\n",
    "    # =====================\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    save_steps = 500\n",
    "    eval_steps = 500\n",
    "    scaler = GradScaler(enabled=True)\n",
    "    scaler_values = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader)):\n",
    "            if accelerator:\n",
    "                with accelerator.accumulate(model):\n",
    "                    with autocast(dtype=torch.bfloat16):\n",
    "                        outputs = model(**batch)\n",
    "                        loss = outputs.loss\n",
    "\n",
    "                    accelerator.backward(loss)\n",
    "\n",
    "                    if accelerator.sync_gradients:\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        global_step += 1\n",
    "            else:\n",
    "                with autocast(dtype=torch.bfloat16):\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                    scaler_values.append(scaler.get_scale())\n",
    "\n",
    "\n",
    "            # Logging\n",
    "            if global_step % 10 == 0:\n",
    "                if accelerator:\n",
    "                    accelerator.print(f\"Step {global_step}: loss = {loss.item():.4f}\")\n",
    "                else:\n",
    "                    print(f\"Step {global_step}: loss = {loss.item():.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            if global_step % save_steps == 0:\n",
    "                save_dir = f\"./llama3-cacao-checkpoints-{gParams.finetune_mode}/checkpoint-{global_step}\"\n",
    "                save_checkpoint(model, tokenizer, save_dir, accelerator)\n",
    "\n",
    "            # Evaluate\n",
    "            if global_step % eval_steps == 0:\n",
    "                eval_loss = evaluate_lowlevel(model, val_loader, accelerator)\n",
    "                if accelerator:\n",
    "                    accelerator.print(f\"Eval loss at step {global_step}: {eval_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Eval loss at step {global_step}: {eval_loss:.4f}\")\n",
    "\n",
    "        # Save at end of epoch\n",
    "        save_dir = f\"./llama3-cacao-checkpoints-{gParams.finetune_mode}/epoch-{epoch+1}\"\n",
    "        save_checkpoint(model, tokenizer, save_dir, accelerator)\n",
    "\n",
    "    # =====================\n",
    "    # Final model save\n",
    "    # =====================\n",
    "    final_save_dir = f\"./llama3-cacao-final-{gParams.finetune_mode}\"\n",
    "    save_checkpoint(model, tokenizer, final_save_dir, accelerator)\n",
    "\n",
    "    # Save the scaler values for analysis\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(scaler_values)\n",
    "    plt.title(\"GradScaler Dynamic Loss Scale over Training Steps\")\n",
    "    plt.xlabel(\"Optimizer Step\")\n",
    "    plt.ylabel(\"Scaling Factor\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "train_model_lowlevel(subset=True, learning_rate=2e-5, gradient_accumulation_steps=4, num_epochs=3)#, gpus_list=gpus_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981e69cb968c878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:15:51.185387Z",
     "start_time": "2025-04-27T11:15:39.162618Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2222832/1114190683.py:115: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# Step 3: Train the model\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m#train_model(subset=True) # Set to True for testing purposes, False for full training\u001B[39;00m\n\u001B[32m      4\u001B[39m \n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m#gpus_list = \"0\" #\"0,1,2,3,4,5,7\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43mtrain_model_lowlevel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m#, gpus_list=gpus_list)\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 123\u001B[39m, in \u001B[36mtrain_model_lowlevel\u001B[39m\u001B[34m(subset, learning_rate, gradient_accumulation_steps, num_epochs, batch_size_train, batch_size_eval)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    121\u001B[39m total_loss = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m123\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43maccumulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m708\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    711\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    712\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    713\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    714\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    763\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m764\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    765\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    766\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 53\u001B[39m, in \u001B[36mcustom_collate_fn\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m     51\u001B[39m new_batch = {}\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m batch[\u001B[32m0\u001B[39m].keys():\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m     new_batch[key] = torch.stack([\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m batch])\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m new_batch\n",
      "\u001B[31mTypeError\u001B[39m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 3: Train the model\n",
    "#train_model(subset=True) # Set to True for testing purposes, False for full training\n",
    "\n",
    "#gpus_list = \"0\" #\"0,1,2,3,4,5,7\"\n",
    "train_model_lowlevel(subset=True, learning_rate=2e-5, gradient_accumulation_steps=4, num_epochs=3)#, gpus_list=gpus_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b402df126ec672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training with accelerate\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,7 accelerate launch --mixed_precision bf16 --multi_gpu train_model.py\n",
    "\n",
    "\n",
    "# OR:\n",
    "# accelerate config then accelerate launch train_model.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
